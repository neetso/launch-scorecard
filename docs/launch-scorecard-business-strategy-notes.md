# Keynote Promise Tracker — Business & Strategy Notes

## Who this is for
Primary: tech-focused analysts and “narrative builders”
- Equity research associates/analysts (especially TMT)
- Tech strategy / corp dev teams
- VC/PE platform teams (AI/cloud)
- Competitive intelligence teams
- Newsletter writers + independent analysts (Stratechery-style)

Secondary:
- Product leaders tracking competitors
- Journalists covering platform shifts
- Developer advocates and ecosystem watchers

---

## What analysts care about (jobs-to-be-done)
1) **Accountability**: what was promised vs what shipped (with evidence).
2) **Timing**: whether it’s on schedule, delayed, or quietly pulled.
3) **Scope truth**: who actually gets it (tier/region/device constraints).
4) **Narrative leverage**: clean tables and deep links they can cite quickly.
5) **Comparisons**: cross-company reliability and “ship velocity” over time.

Analysts don’t pay for “interesting.” They pay for:
- time saved,
- defensible citations,
- repeatable workflow (alerts/exports),
- and unique longitudinal data.

---

## Positioning (clear + non-generic)
This is not “news.” It’s a **delivery ledger**:
> “Every promise from major tech events, whether it shipped, when, and the evidence.”

Differentiator: **normalized commitments + audit trail** (status history + citations).

---

## Competitive landscape (how you fit)
Not trying to replace:
- FactSet / Bloomberg / AlphaSense (broad research platforms)
- App intelligence tools (Sensor Tower, data.ai)
- Roadmap pages (single-company, not cross-company)

You are a specialized layer that answers:
- “Did it actually ship?”
- “How late?”
- “What scope limitations?”
- “What’s the pattern over time?”

---

## GTM plan (realistic wedge)
### Step 1 — Wedge product: “AI & Cloud Ship vs Hype”
Start where evidence is clean:
- OpenAI, AWS, Microsoft, Google Cloud

Deliverables:
- Event scorecards same week as keynote
- Weekly digest: “what moved to GA / what slipped”
- Watchlists + alerts for key topics (AI, agents, inference, copilots, pricing)

### Step 2 — Acquisition via “scorecard SEO” + social proof
- Publish event scorecards as indexable pages (some free rows, rest gated)
- Post charts: “ship rate by company”, “most delayed promises”
- Outreach: newsletters, research communities, analyst Twitter/LinkedIn

### Step 3 — Conversion hooks
- CSV export + “copy citation”
- Slack/email alerts: status change, “still unshipped after 90 days”
- Saved queries + watchlists

### Step 4 — Expansion
- Add NVIDIA, then Apple/Meta
- Add API/data feed for teams
- Offer custom coverage packs (e.g. “Salesforce + Adobe + Oracle”)

---

## Pricing (credible for this category)
You’re a specialist product; price like a specialist workflow tool.

Suggested tiers:
- **Individual**: €29–€79/mo
  - dashboards, scorecards, filters, limited exports
- **Team**: €499–€1,500/mo (5–15 seats)
  - shared watchlists, Slack alerts, full exports, saved queries
- **Enterprise**: €15k–€60k/yr
  - SSO, API feed, custom coverage, SLA, onboarding

Key: make the free layer valuable enough to be cited, but paid layer essential for workflow.

---

## Defensibility (what becomes the moat)
### 1) Normalization + IDs
Atomic commitments with stable URLs become the “canonical references.”

### 2) Evidence graph + status ledger
The immutable StatusHistory tied to evidence is hard to replicate and builds trust.

### 3) Retrospective backfill (compounding advantage)
Years of history create a dataset that’s painful for competitors to recreate.
Backfilling earlier years also becomes SEO inventory.

### 4) Outcome layer (optional but powerful)
“Success” is not in official trackers. Your transparent proxy scoring + evidence bundle becomes proprietary.

What is NOT defensible:
- Scraping alone
- A prompt-only wrapper without stable IDs and history

---

## Why “just prompts” won’t replace this
LLMs can:
- extract and split commitments
- suggest evidence matches

They cannot, by themselves, provide:
- stable, citable identifiers
- consistent status logic over time
- deduplication/versioning
- an auditable evidence ledger
- a reliable historical record

Your product is the **database + governance + publication layer**.

---

## Distribution & marketing angles (what will spread)
- “WWDC: what shipped vs promised (live tracker)”
- “Top 10 most delayed AI features across Big Tech”
- “Ship-rate leaderboard: who overpromises?”
- Earnings season tie-ins: “still preview” vs narrative claims

These work because they’re:
- concrete,
- chartable,
- linkable,
- and repeatable every event season.

---

## Risks & mitigations
### Risk: Apple/Meta ambiguity makes you look wrong
Mitigation:
- Make `PARTIAL` + `confidence` visible
- Show scope constraints explicitly
- Require primary evidence to mark GA

### Risk: Data volume explodes
Mitigation:
- Start with Phase 1 only
- Cap commitments per event by focusing on “top announcements” first

### Risk: Analysts want API/data feed early
Mitigation:
- Offer CSV first, then API for enterprise

---

## First “product proof” milestone
Pick one major event (e.g., Build or re:Invent) and publish:
- event scorecard within 48h
- 30-day follow-up showing what actually reached GA
- a ship-rate chart

If you can do that twice, you have a credible wedge.